
![](../png/video_base.webp)

### 视频
#### Stride
> Stride也可以称之为跨距，指的是图像存储时内存中每行像素所占用的空间。跨距为了能够快速读取一行像素，我们一般会对内存中的图像实现内存对齐，比如16字节对齐。

#### RGB
> 一般来说，我们看到的彩色图像中，都有三个通道: R、G、B通道，有时候还会有Alpha值，代表透明度，通常R、G、B各占8个位，我们称这种图像是8bit图像。

#### YUV
> 对于图像显示器来说，它是通过RGB模型来显示图像的。而在传输图像数据时是使用YUV模型的，因为YUV模型可以节省带宽。所以就需要采集图像时将RGB模型转换成YUV模型，显示时再将YUV模型转换为RGB模型。
>
> 从视频采集与处理的角度来说，一般的视频采集芯片输出的码流一般都是YUV数据流的形式，而从视频处理(例如H.264、MPEG视频编解码)的角度来说，也是在原始YUV码流进行编码和解析;如果采集的资源是RGB的，也需要转换成YUV。
> 
> 使用YUV而非RGB的两个原因:
> 1. yuv提取y亮度信号，可以直接给黑白电视使用，兼容黑白电视
> 2. 人对uv的敏感性小于亮度，这样我们可以适当减少uv的量，进行视频的压缩。所以才会420、422、444等不同的yuv描述。
> YUV颜色编码采用的是**明亮**和**色度**来指定像素的颜色。其中，Y表示明亮度(Luminance、Luma)，而U和V表示色度(Chromiance、Chroma)。YUV主要分YUV 4:4:4, YUV 4:2:2, YUV 4:2:0几种常用类型
>
> YUV格式有两大类: planar和packed。对于plannar的YUV格式，先连续存储所有像素点的Y，紧接着存储所有像素点的U，随后是所有像素点的V。对于packed的YUV格式，每个像素点的Y,U,V是连续交叉存储的。

##### YUV 4:4:4采样
> 意味着Y、U、V三个分量的采样比例相同，所以在生成的图像里，每个像素的三个分量信息都是8bit。
> 
> 例:
> + 假如图像像素为: [Y0 U0 V0]、[Y1 U1 V1]、[Y2 U2 V2]、[Y3 U3 V3]
> + 那么采样的码流为 Y0 U0 V0 Y1 U1 V1 Y2 U2 V2 Y3 U3 V3
> + 最后映射出的像素点依旧为 [Y0 U0 V0]、[Y1 U1 V1]、[Y2 U2 V2]、[Y3 U3 V3]
> 这种采样方式的图像和RGB颜色模型的图像大小是一样的，并没有达到节省宽带的目的

##### YUV 4:2:2采样
> UV分量是Y分量的一半，Y分量和UV分量按照2:1的比例采样，如果水平方向有10个像素点，那么采样了10个Y分量，就只采样5个UV分量
>
> 例:
> + 假如图像像素为: [Y0 U0 V0]、[Y1 U1 V1]、[Y2 U2 V2]、[Y3 U3 V3]
> + 那么采样的码流为: Y0 U0 Y1 V1 Y2 U2 Y3 V3
> + 其中，每采样过一个像素点，都会采样其Y分量，而U、V分量就会间隔一个采集一个
> + 最后映射出的像素点为: [Y0 U0 V1]、[Y1 U0 V1]、[Y2 U2 V3]、[Y3 U2 V3]
> 通过例子可以发现第一个像素点和第二个像素点共用了[U0 V1]分量，第三个和第四个共用了[U2 V3]分量，这样就节省了图像空间(节省了1/3)

##### YUV 4:2:0采样
> YUV 4:2:0并不是只采样U分量，而是指，在每一行扫描时，只扫描一种色度分量(U或V)，和Y分量按照2:1的方式采样。
> 
> 例:
> + 假如图像像素为: 
> + [Y0 U0 V0]、[Y1 U1 V1]、[Y2 U2 V2]、[Y3 U3 V3]
> + [Y4 U4 V4]、[Y5 U5 V5]、[Y6 U6 V6]、[Y7 U7 V7]
> + 采样的码流为: Y0 U0 Y1 Y2 U2 Y3 Y4 V4 Y5 Y6 V6 Y7
> + 其中，每采样过一个像素点，都会采样其Y分量，而U、V分量就会间隔一行按照2:1进行采样
> + 最后映射出的像素为: 
> + [Y0 U0 V4]、[Y1 U0 V4]、[Y2 U2 V6]、[Y3 U2 V6]
> + [Y4 U0 V4]、[Y5 U0 V4]、[Y6 U2 V6]、[Y7 U2 V6]
> 通过YUV 4:2:0采样后的图像节省了一半的存储空间

#### 编码
> 就是按照一定的方法，将信息从一种形式(格式),转换成另一种形式(格式)。
>
> 编码的终极目的，就是为了压缩。各种视频编码方式，都是为了让视频变得体积更小，有利于存储和传输。要实现压缩，就要设计各种算法将视频数据中的冗余信息去除。
>
> 图像一般都是有数据冗余的，主要包括以下4种:
> 1. 空间冗余: 比如说将一个图像划分为一个个16x16的块之后，相邻的块很多时候都有比较明显的相似性，这种就叫空间冗余。
> 2. 时间冗余: 一个帧率为25fps的视频中前后两帧图像相差只有40ms，两张图像的变化是比较小的，相似性很高，这种叫做时间冗余。
> 3. 视频冗余: 我们的眼睛是有视频灵敏度这个东西的。人的眼睛对图像中高频信息的敏感度是小于低频信息的。有的时候去除图像中的一些高频信息，人眼看起来跟不去除高频信息差别不大，这种叫做视沉冗余。
> 4. 信息熵冗余: 我们一般使用Zip等压缩工具去压缩文件，将文件大小减小，这个对图像来说也是可以做的，这种冗余叫信息熵冗余。  
> 各种视频压缩算法就是为了减少上面的这几种冗余。视频编码技术优先消除的目标，就是**空间冗余**和**时间冗余**。

#### 宏块
> 每一帧图像，又是划分成一个个块来进行编码的，这一个个块在H264中叫做宏块，而在VP9、AV1中称之为超级块，其实概念是一样的。宏块大小一般是16x16(H264、VP8)，32x32（H265、VP9）,64x64(H265、VP9、AV1)，128x128(AV1)这几种。

![](../png/video_macro.webp)

#### 帧内预测和帧间预测
> **帧内预测**: 基于同一帧内已编码块预测，构造预测块，计算与当前块的残差，对残差、预测模式等信息进行编码。基主要去除的是空间冗余
> **帧间预测**: 基于一个或多个已编码帧预测，构造预测块，计算与当前块的残差，对残差、预测模式、运动矢量残差、参考图像索引等信息进行编码。基主要去除的是时间冗余

#### 视频传输原理
> 视频是利用人眼视觉暂留的原理，通过播放一系列的图片，使人眼产生运动的感觉。单纯传输视频画面，数据量太大，因此有了H.264视频压缩标准。
>
> 视频里面的原始图像数据会采用H.264编码格式进行压缩，音频采样数据会采用AAC编码格式进行压缩。

##### I帧
> Intra-coded picture(帧内编码图像帧), I帧表示关键帧，解码时只需要本帧数据就可以完成。编码流程:
- 进行帧内预测，决定所采用的帧内预测模式
- 像素值减去预测值，得到残差
- 对残差进行变换和量化
- 变长编码和算术编码
- 重构图像并滤波，得到的图像作为其他帧的参考帧

##### P帧
> Predictive-coded Picture(前向预测编码图像帧), P帧表示的是这一帧和之前的一个关键帧(或P帧)的差别，解码时需要用之前缓存的画面叠加上本帧定义的差别，生成最终画面。特点:
- P帧是I帧后面相隔1~2帧的编码帧
- P帧采用运动补偿的方法传送它与前面的I或P帧的差值及运动矢量(预测误差)
- 解码时必须将I帧中的预测值与预测误差求和后才能重构完整的P帧
- P帧属于前向预测的的帧间编码。它只参考前面的最靠近它的I帧或P帧
- 由于P帧是参考帧，它可能造成编码错误的扩散
- 由于是差值传送，P帧的压缩比较高

##### B帧
> Bidirectionally predicted picture(双向预测编码图像帧), B帧是双向差别帧，也就是B帧记录的是本帧与前后帧的差别，换言之，要解码B帧，不仅要取得之前的缓存画面，还要解码之后的画面，通过前后画面与本帧数据的叠加取得最终的画面。
> B帧压缩率高，但是解码时CPU会比较累。特点:
- B帧是由前面的I或P帧和后面的P帧来进行预测的
- B帧传送的是它与前面的I帧或P帧和后面的P帧之间的预测误差及运动矢量
- B帧是双向预测编码帧
- B帧压缩比最高，因为它只反映丙参考帧间运动主体的变化情况，预测比较准确
- B帧不是参考帧，不会造成解码错误的扩散

##### GOP(序列)和IDR
> 在H264中图像以**序列**为单位进行组织，一个序列是一段图像编码后的数据流
> 一个序列的第一个图像叫做**IDR图像(立即刷新图像)**，IDR图像都是I帧图像。H.264引入IDR图像是为了解码的重同步，当解码器解码到IDR图像时，立即将参考帧队列清空，将已解码的数据全部输出或抛弃，重新查找参数集，开始一个新的序列。
> 在视频编码序列中，GOP即Group of picture(**图像组**),指两个I帧之前的距离，Reference(**参考周期**)指两个P帧之间的距离。两个I帧之间形成一组图片，就是GOP。

![](http://pc-cors.elitb.com/proxy?url=https://img2020.cnblogs.com/blog/653161/202112/653161-20211216165707365-1947946625.png)

##### PTS和DTS
> P帧需要参考前面的I帧或P帧，B帧需要参考前面的I帧和P帧及后面的P帧。这就给解码带来了问题: 先到来的B帧无法立即解码，需要等待它依赖的后面的I、P帧先解码完成，这样一来播放时间与解码时间就不一致了。
- DTS(Decoding Time Stamp): **解码时间戳**, 这个时间戳的意义在于告诉播放器该在什么时候解码这一帧的数据。
- PTS(Presentation Time Stamp): **显示时间戳**, 这个时间戳用来告诉播放器该在什么时候显示这一帧的数据。
> 虽然DTS、PTS是用于指导播放端的行为，但它是在编码时由编码器生成的。例:
```javascript
I B B P B B P
1 2 3 4 5 6 7       // 显示顺序
1 4 2 3 7 5 6       // 编码顺序

I P B B P B B       // 网络推流顺序
1 2 3 4 5 6 7       // 解码顺序 (DTS)
1 4 2 3 7 5 6       // 播放顺序 (PTS)
// 按PTS重新调整解码后的frame(帧)
I B B P B B P
1 3 4 2 6 7 5     // DTS
1 2 3 4 5 6 7     // PTS
```

### 音频

![](../png/audio_base.wepb)

#### 采样
> 所谓采样，即以适当的时间间隔观测模拟信号波形不连续的样本值替换原来的连续信号波形的操作，又称为取样。
>
> 采样的基本原理: 为了复原波形，一次振动中，必须有2个点的采样，人耳能够感觉到的最高频率为20kHz，因此要满足人耳的听觉要求，则需要至少每秒进行40k次采样。

#### 量化
> 在数字音频技术中，把表示声音强弱的模拟电压用数字表示，如0.5V电压用数字20表示，2V电压是80表示。模拟电压的幅度，即使在某电平范围内，仍然可以有无穷多个，如1.2V,1.21V.1.215V...。而用数字来表示音频幅度时，只能把无穷多个电压幅度用限个数字表示。即把某一幅度范围的电压用一个数字表示，这就之为量化。

#### 编码
> 计算机内的基本数制是二进制，为此我们也要把声音数据写成计算机的数据格式，这称之为编码。

### 其它

#### 帧封装(NALU)
> 原始码流是一个接一个的NALU组成的，相当于帧(Frame)

![](../png/video_nalu.webp)

##### SPS和PPS
> 在H.264标准协议中规定了多种不同的NAL Unint类型，其中类型7表示该NAL Unint内保存的数据为SPS。在H.264的各种语法元素中，SPS中的信息至关重要。如果其中的数据丢失或出现错误，那么解码过程很可能会失败。

###### 序列参数集(Sequence Paramater Set， SPS)
> SPS包含编码序列的一些参数。

###### 图像参数集(Picture Paramater Set, PPS)
> 包含编码图像的一些参数

#### 画质相关
> 1. CBR(Constant Bit Rate) 固定比特率，检测每一帧图像的复杂程度，然后计算出码率。如果码率过小，就填充无用数据，使之与指定码率保持一致;如果码率过大，就适当降低码率，也使之与指定码率保持一致。因此，固定码率模式的编码效率比较低。在快速运动画面部分，由于强行降低码率而出现马赛克。
> 2. VBR(Variable Bit Rate) 可变比特率，与固定码率(CBR)模式不同，其码率可以随着图像的复杂程度的不同而变化。因此其编码效率比咬高，快速运动画面的马赛克就很很少。
> 3. CQ(Constant Quality) 固定品质，也是属于可变码率模式，品质越高，图像质量就越好，但码率就越高，生成文件就越大，花费时间就越长。